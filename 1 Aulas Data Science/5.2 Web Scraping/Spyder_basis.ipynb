{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.http import TextResponse, HtmlResponse\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "    name = \"your_spider\"\n",
    "    \n",
    "    # start_requests method\n",
    "    def start_requests( self ):\n",
    "        urls = ['https://www.datacamp.com', \"https://scrapy.org\"]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "            \n",
    "    # parse method\n",
    "    def parse( self, response ):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spiders to get autors:\n",
    "\n",
    "class DCspider( scrapy.Spider ):\n",
    "    name = 'dcspider'\n",
    "  # start_requests method\n",
    "    def start_requests( self ):\n",
    "        \n",
    "        url = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short' \n",
    "        resp = requests.get(url)\n",
    "        resp = TextResponse(body=resp.content, url=url)\n",
    "        \n",
    "        author_names = resp.css('.course-block__author-name::text').extract()\n",
    "        \n",
    "    # Here we will just return the list of Authors\n",
    "        return author_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jonathan Cornelissen',\n",
       " 'Matt Dowle',\n",
       " 'Garrett Grolemund',\n",
       " 'Garrett Grolemund',\n",
       " 'Garrett Grolemund',\n",
       " 'Filip Schouwenaars',\n",
       " 'Gilles Inghelbrecht',\n",
       " 'Nick Carchedi',\n",
       " 'Filip Schouwenaars',\n",
       " 'Filip Schouwenaars',\n",
       " 'Mark Peterson']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = DCspider()\n",
    "foo.start_requests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jonathan Cornelissen\n",
      "Matt Dowle\n",
      "Garrett Grolemund\n",
      "Garrett Grolemund\n",
      "Garrett Grolemund\n",
      "Filip Schouwenaars\n",
      "Gilles Inghelbrecht\n",
      "Nick Carchedi\n",
      "Filip Schouwenaars\n",
      "Filip Schouwenaars\n",
      "Mark Peterson\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    foo = DCspider()\n",
    "    lista = foo.start_requests()\n",
    "    for i in lista:\n",
    "        print(i)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One course description you found is: In this introduction to R, you will master the basics of this beautiful open source language, including factors, lists and data frames. With the knowledge gained in this course, you will be ready to undertake your first very own data analysis. With over 2 million users worldwide R is rapidly becoming the leading programming language in statistics and data science. Every year, the number of R users grows by 40% and an increasing number of organizations are using it in their day-to-day activities. Leverage the power of R by completing this free R online course today!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the Spider class\n",
    "class DCdescr2( scrapy.Spider ):\n",
    "    name = 'dcdescr'\n",
    "  \n",
    "   # start_requests method\n",
    "    def start_requests( self ):\n",
    "        yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  \n",
    "    # First parse method\n",
    "    def parse( self, response ):\n",
    "        links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
    "    # Follow each of the extracted links\n",
    "        for link in links:\n",
    "              yield response.follow( url = link, callback = self.parse_descr )\n",
    "      \n",
    "  # Second parsing method\n",
    "    def parse_descr( self, response ):\n",
    "        # Extract course description\n",
    "        course_descr = response.css( 'p.course__description::text' ).extract_first()\n",
    "        # For now, just yield the course description\n",
    "        yield course_descr\n",
    "\n",
    "\n",
    "        \n",
    "def inspect_spider( s ):\n",
    "    news = s()\n",
    "    try:\n",
    "        req1 = list( news.start_requests() )[0]\n",
    "        html1 = requests.get( req1.url ).content\n",
    "        response1 = TextResponse( url = req1.url, body = html1, encoding = 'utf-8' )\n",
    "        req2 = list( news.parse( response1 ) )[0]\n",
    "        html2 = requests.get( req2.url ).content\n",
    "        response2 = TextResponse( url = req2.url, body = html2, encoding = 'utf-8' )\n",
    "        for d in news.parse_descr( response2 ):\n",
    "            print(\"One course description you found is:\", d )\n",
    "            break\n",
    "    except:\n",
    "        print(\"Oh no! Something is wrong with the code. Keep trying!\")\n",
    "        \n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "# Inspect the spider\n",
    "inspect_spider( DCdescr2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spyder with 2 leves \n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Chapter_Spider(scrapy.Spider):\n",
    "    name = \"dc_chapter_spider\"\n",
    "    \n",
    "    # start_requests method\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url = url_short,\n",
    "                             callback = self.parse_front)\n",
    "    # First parsing method\n",
    "    def parse_front(self, response):\n",
    "        course_blocks = response.css('div.course-block')\n",
    "        course_links = course_blocks.xpath('./a/@href')\n",
    "        links_to_follow = course_links.extract()\n",
    "        for url in links_to_follow:\n",
    "            yield response.follow(url = url,\n",
    "                                  callback = self.parse_pages)\n",
    "  \n",
    "    # Second parsing method\n",
    "    def parse_pages(self, response):\n",
    "        crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "        crs_title_ext = crs_title.extract_first().strip()\n",
    "        ch_titles = response.css('h4.chapter__title::text')\n",
    "        ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "        dc_dict[ crs_title_ext ] = ch_titles_ext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-14 15:55:05 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: scrapybot)\n",
      "2019-10-14 15:55:05 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.6.1, Platform Windows-10-10.0.17763-SP0\n",
      "2019-10-14 15:55:05 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2019-10-14 15:55:05 [scrapy.extensions.telnet] INFO: Telnet Password: 67839894d38fa54b\n",
      "2019-10-14 15:55:05 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-10-14 15:55:06 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-10-14 15:55:06 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-10-14 15:55:06 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-10-14 15:55:06 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-10-14 15:55:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-10-14 15:55:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-10-14 15:55:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short> (referer: None)\n",
      "2019-10-14 15:55:07 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://assets.datacamp.com/courses/predicting-customer-churn-in-python> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-10-14 15:55:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://assets.datacamp.com/courses/predicting-customer-churn-in-python>: HTTP status code is not handled or not allowed\n",
      "2019-10-14 15:55:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/60f4cecb02a7e8e78c74643f095e3c913348da9b/dplyr-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-10-14 15:55:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/844f6aa70cafd81cbc92f344baff641173604229/reporting-with-r-markdown> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-10-14 15:55:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/6d40286d4abe480763ff8e8ac2246c01861f8c27/intermediate-r-practice> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-10-14 15:55:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/77b65bcd1c82bf793e2de583c861a077dcec2246/ggvis-data-visualization-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-10-14 15:55:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/97b54f43a3d96a7c35defb3c757ecf5471152941/intermediate-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-10-14 15:55:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/c38792ee5ab59361c958c1b0b4453aa1385acd10/cleaning-data-in-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-10-14 15:55:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/9f9f9be002e7c66df8c1733b8796943fa77b2236/introduction-to-machine-learning-with-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-10-14 15:55:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/5938f64803ca4a1275f63bf68ac3d300cd9f1c4a/intro-to-python-for-data-science> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-10-14 15:55:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/ffbc79c0169a150a45a0b503bd19662cb4d44790/free-introduction-to-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-10-14 15:55:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/537eff88062cdcedb77b51d1622aa7675e2baf21/data-table-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-10-14 15:55:08 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-10-14 15:55:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 5215,\n",
      " 'downloader/request_count': 12,\n",
      " 'downloader/request_method_count/GET': 12,\n",
      " 'downloader/response_bytes': 2463299,\n",
      " 'downloader/response_count': 12,\n",
      " 'downloader/response_status_count/200': 11,\n",
      " 'downloader/response_status_count/403': 1,\n",
      " 'elapsed_time_seconds': 2.968159,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 10, 14, 18, 55, 8, 996714),\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/403': 1,\n",
      " 'log_count/DEBUG': 12,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 12,\n",
      " 'scheduler/dequeued': 12,\n",
      " 'scheduler/dequeued/memory': 12,\n",
      " 'scheduler/enqueued': 12,\n",
      " 'scheduler/enqueued/memory': 12,\n",
      " 'start_time': datetime.datetime(2019, 10, 14, 18, 55, 6, 28555)}\n",
      "2019-10-14 15:55:08 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Chapter_Spider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Manipulation in R with dplyr \n",
      "\n",
      "Introduction to dplyr and tbls\n",
      "Filter and arrange\n",
      "Group_by and working with databases\n",
      "Select and mutate\n",
      "Summarize and the pipe operator\n",
      "Introduction to dplyr and tbls\n",
      "Select and mutate\n",
      "Filter and arrange\n",
      "Summarize and the pipe operator\n",
      "Group_by and working with databases\n",
      "\n",
      " \n",
      "\n",
      "Reporting with R Markdown \n",
      "\n",
      "Authoring R Markdown Reports\n",
      "Embedding Code\n",
      "Compiling Reports\n",
      "Configuring R Markdown (optional)\n",
      "\n",
      " \n",
      "\n",
      "Intermediate R - Practice \n",
      "\n",
      "Conditionals and Control Flow\n",
      "Loops\n",
      "Functions\n",
      "The apply family\n",
      "Utilities\n",
      "\n",
      " \n",
      "\n",
      "Data Visualization in R with ggvis \n",
      "\n",
      "The Grammar of Graphics\n",
      "Transformations\n",
      "Customizing Axes, Legends, and Scales\n",
      "Lines and Syntax\n",
      "Interactivity and Layers\n",
      "The Grammar of Graphics\n",
      "Lines and Syntax\n",
      "Transformations\n",
      "Interactivity and Layers\n",
      "Customizing Axes, Legends, and Scales\n",
      "\n",
      " \n",
      "\n",
      "Intermediate R \n",
      "\n",
      "Conditionals and Control Flow\n",
      "Functions\n",
      "Utilities\n",
      "Loops\n",
      "The apply family\n",
      "Conditionals and Control Flow\n",
      "Loops\n",
      "Functions\n",
      "The apply family\n",
      "Utilities\n",
      "\n",
      " \n",
      "\n",
      "Cleaning Data in R \n",
      "\n",
      "Introduction and exploring raw data\n",
      "Preparing data for analysis\n",
      "Tidying data\n",
      "Putting it all together\n",
      "Introduction and exploring raw data\n",
      "Tidying data\n",
      "Preparing data for analysis\n",
      "Putting it all together\n",
      "\n",
      " \n",
      "\n",
      "Introduction to Machine Learning \n",
      "\n",
      "What is Machine Learning\n",
      "Classification\n",
      "Clustering\n",
      "Performance measures\n",
      "Regression\n",
      "What is Machine Learning\n",
      "Performance measures\n",
      "Classification\n",
      "Regression\n",
      "Clustering\n",
      "\n",
      " \n",
      "\n",
      "Intro to Python for Data Science \n",
      "\n",
      "Python Basics\n",
      "Functions and Packages\n",
      "Python Lists\n",
      "NumPy\n",
      "Python Basics\n",
      "Python Lists\n",
      "Functions and Packages\n",
      "NumPy\n",
      "\n",
      " \n",
      "\n",
      "Introduction to R \n",
      "\n",
      "Intro to basics\n",
      "Vectors\n",
      "Matrices\n",
      "Factors\n",
      "Data frames\n",
      "Lists\n",
      "\n",
      " \n",
      "\n",
      "Data Analysis in R, the data.table Way \n",
      "\n",
      "Data.table novice\n",
      "Data.table yeoman\n",
      "Data.table expert\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a preview of courses\n",
    "for i in dc_dict:\n",
    "    print(i, '\\n')\n",
    "    lista = dc_dict[i]\n",
    "    for j in lista:\n",
    "        print(j)\n",
    "    print('\\n \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Description_Spider(scrapy.Spider):\n",
    "    name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "    def parse_front(self, response):\n",
    "        course_blocks = response.css('div.course-block')\n",
    "        course_links = course_blocks.xpath('./a/@href')\n",
    "        links_to_follow = course_links.extract()\n",
    "        for url in links_to_follow:\n",
    "            yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  \n",
    "    # Second parsing method\n",
    "    def parse_pages(self, response):\n",
    "    # Create a SelectorList of the course titles text\n",
    "        crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    # Extract the text and strip it clean\n",
    "        crs_title_ext = crs_title.extract_first().strip()\n",
    "    # Create a SelectorList of course descriptions text\n",
    "        crs_descr = response.css( 'p.course__description::text' )\n",
    "    # Extract the text and strip it clean\n",
    "        crs_descr_ext = crs_descr.extract_first().strip()\n",
    "    # Fill in the dictionary\n",
    "        dc_dict[crs_title_ext] = crs_descr_ext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-14 16:49:33 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: scrapybot)\n",
      "2019-10-14 16:49:33 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.6.1, Platform Windows-10-10.0.17763-SP0\n",
      "2019-10-14 16:49:33 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2019-10-14 16:49:33 [scrapy.extensions.telnet] INFO: Telnet Password: 820e890f33c2d627\n",
      "2019-10-14 16:49:33 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-10-14 16:49:33 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-10-14 16:49:33 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-10-14 16:49:33 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-10-14 16:49:33 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-10-14 16:49:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-10-14 16:49:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-160-16dbf0184aa0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprocess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrawlerProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDC_Description_Spider\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdc_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self, stop_after_crawl)\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[0mtp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjustPoolsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxthreads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'REACTOR_THREADPOOL_MAXSIZE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[0mreactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddSystemEventTrigger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'before'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shutdown'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[0mreactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocking call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_dns_resolver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1281\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1282\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1283\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \"\"\"\n\u001b[0;32m   1261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_installSignalHandlers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1262\u001b[1;33m         \u001b[0mReactorBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    763\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReactorAlreadyRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_startedBefore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReactorNotRestartable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stopped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Description_Spider)\n",
    "process.start()\n",
    "\n",
    "dc_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
